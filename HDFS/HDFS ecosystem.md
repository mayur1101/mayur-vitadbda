# **HADOOP ECOSYSTEM**
<img src="/Images/hdfs ecosystem.png" width=700>

The various tools are used in the Hadoop ecosystems.

## 1)HDFS(Hadoop distributed file system)
- Stores different types of large data sets(i.e. structured,unstructured and semi structured data).
- Stores data across various nodes and maintains the log file about the stored data(metadata).
- HDFS has two main components i.e. NameNode and DataNode.

## 2)YARN(Yet Another Resource Negotiator)
- Yet Another Resource Negotiator, as the name implies, YARN is the one who helps to manage the resources across the clusters.
In short, it performs scheduling and resource allocation for the Hadoop System.
- Consists of three major components i.e.
1) Resource Manager
2) Nodes Manager
3) Application Manager
- **Resource manager** has the privilege of allocating resources for the applications in a system.
- **Node managers work** on the allocation of resources such as CPU, memory, bandwidth per machine and later on acknowledges the resource manager. 
- **Application manager** works as an interface between the resource manager and node manager and performs negotiations as per the requirement of the two.

## 3)MapReduce(Data Processing Using Programming)
- Core component in Hadoop Ecosystem for processing.
- By making the use of distributed and parallel algorithms, MapReduce makes it possible to carry over the processing’s logic and helps
to write applications which transform big data sets into a manageable one.
- MapReduce makes the use of two functions i.e. Map() and Reduce() whose task is:

**Map()** performs sorting and filtering of data and thereby organizing them in the form of group. Map generates a key-value pair based result which is later on processed
by the Reduce() method.

**Reduce()** as the name suggests does the summarization by aggregating the mapped data. In simple, Reduce() takes the output generated by Map() 
as input and combines those tuples into smaller set of tuples.

## 4)Apache pig(Data Processing Service using Query)
- Pig was basically developed by Yahoo which works on a pig Latin language, which is Query based language similar to SQL.
- Pig has two parts:Pig Latin,the language and the pig runtime,for the execution envoirnment.
- **1 line of pig latin=approx. 100 lines of Map-reduce job.**
- The compiler internally converts pig latin to MapReduce.
- It gives you a platform for building data flow for ETL(Extract,Transform,Load).
- PIG first loads the data,then performs various functions like grouping,filtering,joining,sorting,etc and finally dumps the data on the screen or stores in HDFS.

## 5)Hive(Data Processing Service using query)
- With the help of SQL methodology and interface, HIVE performs reading and writing of large data sets. However, its query language is called as HQL (Hive Query Language).
- It is highly scalable as it allows real-time processing and batch processing both. Also, all the SQL datatypes are supported by Hive thus, making the query processing easier.
- Similar to the Query Processing frameworks, HIVE too comes with two components: JDBC Drivers and HIVE Command Line.
- JDBC, along with ODBC drivers work on establishing the data storage permissions and connection whereas HIVE Command line helps in the processing of queries.

## 6)Mahout(Machine Learning Library written in java)
- Provides an envoirnment for creating machine learning applications.
- It performs collabrative filtering,clustering and classification.
- Provides a command line to invoke various algorithms.
- It is a perdefined set of library which already contains different inbuilt algorithms for different use cases.

## 7)Spark(In-memory Data Processing)
- A framework for real time data analytics in the distributed computing envoirnment.
- Written in Scala and was orignally developed at the university of California,Berkeley.
- It executes in-memory computations to increase the speed of data processing over map-reduce.
- 100x faster than hadoop large sclae data processing(apache map reduce) by exploiting in-memory computations.

## 8)Apache Hbase(NoSQL Database)
- It’s a NoSQL database which supports all kinds of data and thus capable of handling anything of Hadoop Database. It provides capabilities of Google’s BigTable, thus able to work on Big Data sets effectively.
- At times where we need to search or retrieve the occurrences of something small in a huge database, the request must be processed within a short quick span of time. 
At such times, HBase comes handy as it gives us a tolerant way of storing limited data.
- It is written in java and HBase applications can be written in REST,Avro and Thrift Apis.

## 9)Apache Drill(SQL on Hadoop)
- An open source application which works with distributed envoirnment to to analyze large data sets.
- Follows the ANSI SQL.
- Suports different kinds NoSQL databases and file systems.
- For example:MongoDB,Google cloud storage,Hbase.
- Using the apache drill we can excecute teh Hbase,Hive,Pig any database analysis using single query this is the power of apache drill.
- Combines a variety of data stores just by using a single query.

## 10)Apache oozie(JOb scheduler)
- Oozie is a job scheduler in Hadoop ecosystem.
- Two kinds of oozie jobs:Oozie workflow and Oozie coordinator.
- **Oozie workflow:** Sequential set of actions to be executed.For eg:-If the job is executed every after one hour then we 
can set the time and number of tasks in Ooziew workflow and Oozie workflow do the job automatically every after 1 hour.
- **Oozie Coordinator:** Oozie jobs which are triggered when the data is available to it or even triggerd based on time.

## 11)Apache Flume(Data Ingesting Service)
- Ingesting unstructured and semi-structured data into HDFS.
- It helps in collecting,aggregating and moving large amount of datasets.
- It helps us to ingest onilne streaming data from various sources like network traffic,social media,email messages,log files etc. in HDFS.

## 12)Apache Sqoop(Data Ingesting Service)
- Another data ingesting service.
- Sqoop con import as well as export data from RDBMS.i.e. the structured data from RDBMS can be stored in HDFS and vice-aversa.
- Flume only ingests unstructured data or semi-structured data into hdfs but Sqoop ingests structured data.

## 13)Apache Solr and Lucene
- Two services are used for searching and indexing in Hadoop Ecosystem.
- Apache Lucene is based on java,which also helps in spell checking.
- Apache Lucene is the engine,Apache Solr is the complete application built around lucene.
- Solr uses Apache Lucene Java Search library for searching and indexing.

## 14)ZooKeeper(Coordinator)
- An open source server which enables high reliable distributed co ordination.
- Because of ZooKeeper the other services which are used in Hadoop ecosystem can communicate with each other synchronously.Without ZooKeeper hadoop system fails.
- It performs synchronization,configuration maintainence,grouping and naming.

## 15)Apache Ambari(Cluster Manager)
- It is a software for provisioning,managing and monitoring Apache Hadoop clusters.
- After installing the Hadoop cluster we install various services in cluster like hive,Hbase,Pig etc that services is managed by Apache Ambari.
- It monitors health and status of the hadoop cluster.
- It is paid software.




